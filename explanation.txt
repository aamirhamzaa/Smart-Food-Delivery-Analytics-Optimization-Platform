Smart Food Delivery Analytics & Optimization Platform — Complete Explanation

Part 1: Brief Explanation of Each Code File

1. notebooks/generate_data.py — Dataset Generator

This script creates 1,200 fake but realistic food delivery orders. It defines 15 restaurants in Bangalore with GPS coordinates, 3 customer areas (Downtown, Suburbs, Business District), 50 delivery partners with base ratings, and 4 weather types. For each order, it randomly picks a restaurant, customer location, weather, partner, and hour. The delivery time is calculated realistically using a formula: base time from distance + weather penalty + peak hour penalty + partner skill bonus + food prep time + random noise. It outputs delivery_data.csv with 17 columns.

Key concept: The data isn't random noise — it has built-in patterns (e.g., storms add 15 min, peak hours add 7 min), which is what makes the later analytics and ML meaningful.

2. docker-compose.yml — Hadoop/Hive Infrastructure

This YAML file defines a mini big data cluster using Docker containers: a Hadoop NameNode (manages file system), a DataNode (stores data), a Hive Server (runs SQL queries on big data), a Hive Metastore (tracks table schemas), and a PostgreSQL database (backs the metastore). Running docker-compose up -d spins up all 5 containers that talk to each other automatically.

Key concept: This simulates a real enterprise big data environment where delivery data would live in HDFS and be queried through Hive.

3. hive_queries/queries.hql — Hive SQL Queries

This file contains 6 HiveQL queries that run on the Hadoop cluster:

    Query 1: Average delivery time broken down by weather AND partner rating tier (High/Medium/Low)
    Query 2: Revenue at risk from delayed orders (>40 min) per customer area, with estimated churn loss at 15%
    Query 3: Classifies all 50 partners into Premium/Standard/Training tiers based on their rating and speed
    Query 4: Compares peak vs off-peak hours on delivery time, order value, and revenue
    Query 5: Analyzes each food type's delivery time range and average order value
    Query 6: Groups orders by distance bucket (Short/Medium/Long) within each area

Key concept: These are the same analytical queries a data engineer would write in a production Hive warehouse.

4. notebooks/hive_processing.py — Python Fallback for Hive

This script replicates all 6 Hive queries using Pandas in case Docker/Hive doesn't work on your machine. It reads the CSV, runs the same groupby/aggregation logic, prints the results, and saves each query output as a CSV in output/reports/. The logic is identical — grouping by weather + rating tier, calculating revenue at risk, classifying partners, etc.

Key concept: Shows you understand both the Hive SQL approach AND can implement the same logic in Python. Both are documented for your project.

5. notebooks/analytics.py — Business Intelligence Engine

This is where custom business metrics are calculated:

    Efficiency Score = (5 - DeliveryTime/10) × PartnerRating × WeatherFactor — A composite score combining speed, quality, and conditions
    Revenue Loss = For every delayed order (>40 min), 15% of the order value is flagged as potential churn loss
    Route Optimization Score = Combines distance efficiency and time efficiency into a 0–100 score
    Customer Satisfaction Index = Weighted formula: 60% delivery speed + 40% partner rating, scaled to 0–100
    Partner Utilization = Orders per unique active hour — shows who's being used efficiently

It also produces weather impact analysis, area-wise performance, and saves an enriched dataset (delivery_data_enriched.csv) with all new columns added. Every subsequent script reads this enriched file.

Key concept: This transforms raw data into business-meaningful KPIs that executives can act on.

6. notebooks/visualizations.py — Statistical Charts (8 charts)

Creates 8 publication-quality charts using Matplotlib and Seaborn:
Chart	What It Shows
01 Weather Impact	Bar chart of avg delivery time by weather + line overlay of revenue loss
02 Partner Efficiency	Scatter plot: rating vs time, bubble size = order count, colored by tier
03 Time Heatmap	Hour × Day Type heatmap showing when deliveries are slowest
04 Food Type Boxplot	Distribution of delivery times per food category
05 Peak Comparison	3 side-by-side bars comparing peak vs off-peak on time, value, efficiency
06 Distance Correlation	Regression scatter of distance vs delivery time with correlation coefficient
07 Area Comparison	Grouped bars comparing all 3 areas on time, satisfaction, delay %, efficiency
08 Hourly Analysis	Dual-axis: bar for order count + line for avg delivery time by hour

Key concept: matplotlib.use("Agg") at the top allows chart generation without a display (headless mode), which is why it works on servers and saves to PNG files.
7. notebooks/geospatial.py — Interactive Maps (4 maps)

Creates 4 interactive HTML maps using Folium (a Python wrapper for Leaflet.js):

    Map 1 — Delivery Heatmap: Red/orange zones show where delayed deliveries concentrate geographically
    Map 2 — Restaurant Performance: Each restaurant is a circle marker colored green/orange/red based on speed, sized by order volume, with popup details
    Map 3 — Route Analysis: Lines drawn from restaurant to customer for the 50 slowest deliveries, color-coded from green (faster) to red (slowest)
    Map 4 — Restaurant Clusters: MarkerCluster groups nearby restaurants, plus a delivery density heatmap layer with toggle controls

Key concept: These are real interactive maps you can zoom, click, and toggle layers — not static images. They open in any browser.
8. notebooks/dashboard.py — Executive Dashboard (Plotly)

Builds a 4×3 grid dashboard using Plotly with 12 panels:

    Row 1: Three KPI indicators — Revenue at Risk (number + delta), Avg Delivery Time (gauge with green/yellow/red zones), Top Partner (rating display)
    Row 2: Three more indicators — Delay Rate %, Satisfaction Score, Total Orders
    Row 3: Weather bar chart, Partner Rating histogram, Hourly order volume line chart
    Row 4: Food type revenue donut/pie chart, Area performance bars, Efficiency score distribution

It also generates system alerts (e.g., "X orders exceeded 30 min", "Y partners below 3.0 rating").

Key concept: This is the kind of real-time command center dashboard that operations managers use. It's a single self-contained HTML file.
9. notebooks/predictive_model.py — Machine Learning

This script trains 3 regression models to predict delivery time:

    Linear Regression — Simple baseline; assumes a straight-line relationship between features and delivery time
    Random Forest — Builds 100 decision trees on random data subsets and averages their predictions; captures non-linear patterns
    Gradient Boosting — Builds trees sequentially where each tree corrects the errors of the previous one; typically most accurate

Process:

    Encodes categorical variables (Weather, FoodType, Area, DayType) into numbers using LabelEncoder
    Selects 9 features (distance, rating, hour, peak, value, weather, food, area, day)
    Splits data 80/20 into train/test sets
    Trains all 3 models, evaluates on MAE, RMSE, R²
    Saves feature importance chart and model comparison chart
    Runs 5 sample predictions on realistic scenarios
    Generates hourly staffing recommendations (orders ÷ 3 = partners needed)

The bug we fixed: In the staffing section, row['orders'] was a float but the format string used :3d (integer format). Wrapping it in int() resolved the ValueError.
10. notebooks/generate_report.py — Final Business Report

Generates a complete text-based executive report with 7 sections:

    Executive Summary (all KPIs in one place)
    Weather Impact Analysis (per-condition breakdown + recommendation)
    Partner Performance (tier distribution, top 5, bottom 5)
    Area-Wise Performance (time, delay rate, revenue per area)
    Food Type Insights (ranked by delivery time)
    Action Plan — 3 concrete recommendations with projected financial impact
    Technical Methodology (documents the entire pipeline)

Key concept: This turns all your analysis into a story that a non-technical stakeholder can read and act on.
11. run_all.py — Master Pipeline Script

Runs all 8 scripts in sequence using Python's subprocess module. It times each step, reports success/failure, and prints a summary of all output file locations at the end. If any script fails, it stops the pipeline.

Key concept: This is a simple orchestrator — in production, you'd use Airflow or similar, but this achieves the same sequential execution.
Part 2: Complete Step-by-Step From Beginning to End

Step 1 — Installed Python libraries: pandas, numpy, matplotlib, seaborn, plotly, folium, scikit-learn, geopy, openpyxl.

Step 2 — Installed Docker Desktop and pulled Hadoop/Hive Docker images for the big data component.

Step 3 — Created the delivery-project/ folder with the docker-compose.yml file defining 5 containers (NameNode, DataNode, Hive Server, Hive Metastore, PostgreSQL).

Step 4 — Ran docker-compose up -d to spin up the Hadoop/Hive cluster.

Step 5 — Created the project folder structure: data/, notebooks/, output/maps/, output/charts/, output/reports/, hive_queries/.

Step 6 — Created and ran generate_data.py to produce 1,200 realistic delivery orders saved as delivery_data.csv.

Step 7 — Copied the CSV into the Hadoop NameNode container and loaded it into HDFS.

Step 8 — Created queries.hql with 6 HiveQL queries for weather analysis, revenue risk, partner tiers, peak/off-peak, food types, and distance buckets.

Step 9 — Ran the Hive queries on the cluster (or used hive_processing.py as the Python fallback doing the same logic with Pandas). Saved results to CSV.

Step 10 — Created and ran analytics.py to compute business metrics: Efficiency Score, Revenue Loss, Route Optimization Score, Customer Satisfaction Index, and Partner Utilization. Saved the enriched dataset.

Step 11 — Created and ran visualizations.py to generate 8 statistical charts (weather impact, partner scatter, heatmap, boxplots, peak comparison, distance regression, area comparison, hourly analysis). Saved as PNGs.

Step 12 — Created and ran geospatial.py to build 4 interactive Folium maps (delay heatmap, restaurant performance, route analysis, clustering). Saved as HTML files.

Step 13 — Created and ran dashboard.py to build the Plotly executive dashboard with 12 panels (KPI gauges, charts, distributions) plus system alerts. Saved as HTML.

Step 14 — Created and ran predictive_model.py to train 3 ML models (Linear Regression, Random Forest, Gradient Boosting), evaluate them, generate feature importance and comparison charts, run sample predictions, and produce staffing recommendations.

Step 15 — Fixed the formatting bug in predictive_model.py where row['orders'] was a float being formatted with :3d. Wrapped it in int().

Step 16 — Created and ran generate_report.py to produce the final executive business report with KPIs, analysis, and 3 actionable recommendations.

Step 17 — Created run_all.py as a master script to execute the entire pipeline in one command.

Final output: A complete end-to-end data engineering and analytics platform with big data processing (Hadoop/Hive), 6 SQL queries, 5 custom business metrics, 8 statistical charts, 4 interactive maps, 1 executive dashboard, 3 trained ML models, and a full business report — all runnable with a single python run_all.py command.